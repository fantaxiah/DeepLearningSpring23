{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIDevH5F7oh+Ma0isfZkhL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fantaxiah/DeepLearningSpring23/blob/main/DLProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzVBN3qoYcCX"
      },
      "outputs": [],
      "source": [
        "# import module\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import xlrd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import shutil\n",
        "import tensorflow as tf \n",
        "\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.decomposition import FastICA\n",
        "\n",
        "# import squarify\n",
        "\n",
        "from keras.models import Model #import model  \n",
        "from keras.layers import Dense, Input\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Conv2D,MaxPooling2D,Flatten,AveragePooling2D,Dropout,BatchNormalization,Activation\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from xlsx file\n",
        "def Load_Data(fname):\n",
        "    ExcelFile = xlrd.open_workbook(fname)\n",
        "\n",
        "    # Read training data\n",
        "    Sheet_1 = ExcelFile.sheet_by_index(0)\n",
        "    Mtraindatalist = list()\n",
        "    for i in range(Sheet_1.ncols-1):\n",
        "        tpcol = Sheet_1.col_values(i+1, 1)\n",
        "        Mtraindatalist.append(tpcol)\n",
        "\n",
        "    Mtraindata = np.array(Mtraindatalist)\n",
        "\n",
        "    # Read training label\n",
        "    Sheet_1 = ExcelFile.sheet_by_index(1)\n",
        "    Vtrainlabel = np.array(Sheet_1.col_values(0,1))\n",
        "\n",
        "    # Read validation data\n",
        "    Sheet_1 = ExcelFile.sheet_by_index(2)\n",
        "    Mvaldatalist = list()\n",
        "    for i in range(Sheet_1.ncols-1):\n",
        "        tpcol = Sheet_1.col_values(i+1, 1)\n",
        "        Mvaldatalist.append(tpcol)\n",
        "\n",
        "    Mvaldata = np.array(Mvaldatalist)\n",
        "\n",
        "    # Read val label\n",
        "    Sheet_1 = ExcelFile.sheet_by_index(3)\n",
        "    Vvallabel = np.array(Sheet_1.col_values(0,1))\n",
        "\n",
        "    Mtraindata = Mtraindata.T\n",
        "    Mvaldata = Mvaldata.T\n",
        "    \n",
        "    # Return matrix is genes*samples\n",
        "    return Mtraindata, Vtrainlabel, Mvaldata, Vvallabel"
      ],
      "metadata": {
        "id": "giz_SR0oaw_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Oversampling by SMOTE\n",
        "# DataSet should be genes*samples\n",
        "def Over_Sampling(DataSet, DataLabel):\n",
        "    X_res, y_res = SMOTE(random_state=3).fit_resample(DataSet.T, DataLabel)\n",
        "    \n",
        "    # Return matrix is genes*samples\n",
        "    return X_res.T, y_res"
      ],
      "metadata": {
        "id": "0GRaHq_xbJ3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Min_Max normalization\n",
        "# DataSet should be genes*samples\n",
        "def MinMax_Norm(DataSet):\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    min_max_scaler.fit(DataSet.T)\n",
        "\n",
        "    DataSet = min_max_scaler.transform(DataSet.T)\n",
        "    \n",
        "    # Return matrix is genes*samples\n",
        "    return DataSet.T"
      ],
      "metadata": {
        "id": "ZxDAOEJPbMNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Topic Model\n",
        "# geneArray should be genes*samples\n",
        "def Topic_Model(geneArray, n_start = 30, n_end = 150, interval = 4, n_iter = 30):\n",
        "\n",
        "    trans_geneArray = geneArray.T\n",
        "    \n",
        "    n_topics = range(n_start, n_end, interval)\n",
        "    perplexityLst = [1.0]*len(n_topics)\n",
        "      \n",
        "    lda_models = []\n",
        "\n",
        "    for idx, n_topic in enumerate(n_topics):\n",
        "        print (\"Start topic searching...: %d in %d\"%(n_topic, n_end))\n",
        "        lda = LatentDirichletAllocation(n_components=n_topic,\n",
        "                                    max_iter=n_iter,\n",
        "                                    learning_method='batch',\n",
        "                                    n_jobs=-1,\n",
        "                                    random_state=100,\n",
        "#                                    evaluate_every=200,\n",
        "                                    perp_tol=0.1, #default                                       \n",
        "                                    doc_topic_prior=0.4, #default\n",
        "                                    topic_word_prior=0.7, #default\n",
        "                                    verbose=1)\n",
        "        lda.fit(trans_geneArray)\n",
        "        perplexityLst[idx] = lda.perplexity(trans_geneArray)\n",
        "        lda_models.append(lda)\n",
        "        \n",
        "    best_index = perplexityLst.index(min(perplexityLst))\n",
        "    best_n_topic = n_topics[best_index]\n",
        "    best_model = lda_models[best_index]\n",
        "    print (\"The best topic number: %d\"%(best_n_topic))\n",
        "    \n",
        "    # return matrix is nTopics x Genes\n",
        "    return best_model.components_, perplexityLst"
      ],
      "metadata": {
        "id": "y5j9Fv6sbRzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build FastICA Model\n",
        "# geneArray should be genes*samples\n",
        "def FastICA_Model(geneArray, nComp = 32):\n",
        "\n",
        "    np.random.seed(100)  # for reproducibility\n",
        "    \n",
        "    # Compute ICA  (reconstruct:  decomp_components * decomp_mixarray.T)\n",
        "    ica = FastICA(n_components=nComp, whiten = True, random_state = 100)\n",
        "    decomp_components = ica.fit_transform(geneArray)  # Calculate components_: Genes x nComps\n",
        "    # decomp_mixarray = ica.mixing_  # Get estimated mixing matrix: samples x % of nComps\n",
        "\n",
        "    final_weights = np.abs(decomp_components)   # Genes x nComps\n",
        "    \n",
        "    # Competion Model\n",
        "    n_genes = final_weights.shape[0]\n",
        "    n_comps = final_weights.shape[1]\n",
        "\n",
        "    for i in range(n_genes):\n",
        "        max_weight = np.max(final_weights[i,:])\n",
        "    \n",
        "        for j in range(n_comps):\n",
        "            if final_weights[i,j] != max_weight:\n",
        "                final_weights[i,j] = 0\n",
        "        #end_for_j  \n",
        "    #end_for_i\n",
        "    \n",
        "    # return matrix is nComps x Genes\n",
        "    return final_weights.T"
      ],
      "metadata": {
        "id": "S2U3LzRhbWtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build AutoEncoder Model\n",
        "# geneArray should be genes*samples    \n",
        "def Auto_Encoder(geneArray, encoding_dim = 32, epch=50, batchsize = 128):\n",
        "    \n",
        "    np.random.seed(100)  # for reproducibility\n",
        "    x_train = geneArray.T\n",
        "    \n",
        "    len_feature = x_train.shape[1]\n",
        "    # this is our input placeholder  \n",
        "    input_img = Input(shape=(len_feature,))\n",
        "\n",
        "    # coding layers  \n",
        "    encoded = Dense(1280, activation='relu', name='encode_layer1')(input_img)\n",
        "    encoded = Dense(320, activation='relu', name='encode_layer2')(encoded)\n",
        "    encoded = Dense(128, activation='relu', name='encode_layer3')(encoded)\n",
        "    encoded = Dense(64, activation='relu', name='encode_layer4')(encoded)\n",
        "    encoder_output = Dense(encoding_dim)(encoded)\n",
        "\n",
        "    # decoding layers  \n",
        "    decoded = Dense(64, activation='relu', name='decode_layer1')(encoder_output)\n",
        "    decoded = Dense(128, activation='relu', name='decode_layer2')(decoded)\n",
        "    decoded = Dense(320, activation='relu', name='decode_layer3')(decoded)\n",
        "    decoded = Dense(1280, activation='relu', name='decode_layer4')(decoded)\n",
        "    decoded = Dense(len_feature, activation='tanh')(decoded)\n",
        "\n",
        "    # autoencoder model \n",
        "    autoencoder = Model(inputs=input_img, outputs=decoded)\n",
        "  \n",
        "    # encoder model\n",
        "    # encoder = Model(inputs=input_img, outputs=encoder_output)\n",
        "\n",
        "    # compile autoencoder  \n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # training  \n",
        "    autoencoder.fit(x_train, x_train, epochs=epch, batch_size=batchsize, shuffle=True)  \n",
        "\n",
        "    autoencoder.save_weights('coder_weights.h5')\n",
        "\n",
        "    # obtained the weights\n",
        "    l1 = autoencoder.get_layer('encode_layer1').get_weights()\n",
        "    l2 = autoencoder.get_layer('encode_layer2').get_weights()\n",
        "    l3 = autoencoder.get_layer('encode_layer3').get_weights()\n",
        "    l4 = autoencoder.get_layer('encode_layer4').get_weights()\n",
        "\n",
        "    # Output weights\n",
        "    a1 = l1[0]\n",
        "    a2 = l2[0]\n",
        "    a3 = l3[0]\n",
        "    a4 = l4[0]\n",
        "\n",
        "    final_weights = np.dot(np.dot(np.dot(a1, a2), a3), a4) # Genes x n_pcs\n",
        "    final_weights = final_weights - np.min(final_weights) # Genes x n_pcs\n",
        "    \n",
        "    n_genes = final_weights.shape[0]\n",
        "    n_comps = final_weights.shape[1]\n",
        "\n",
        "    for i in range(n_genes):\n",
        "        max_weight = np.max(final_weights[i,:])\n",
        "    \n",
        "        for j in range(n_comps):\n",
        "            if final_weights[i,j] != max_weight:\n",
        "                final_weights[i,j] = 0\n",
        "        #end_for_j\n",
        "  \n",
        "    #end_for_i\n",
        "    \n",
        "    # return matrix is nComps x Genes\n",
        "    return final_weights.T "
      ],
      "metadata": {
        "id": "E6DUd2vDbacc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use pretrained weights for coding\n",
        "# the newwork structure should be same as that in Auto_Encode function\n",
        "def Auto_Encoder_Pretrained(geneArray, filename_pretrained_weights, encoding_dim = 32):    \n",
        "    \n",
        "    x_train = geneArray.T\n",
        "    \n",
        "    len_feature = x_train.shape[1]\n",
        "    # this is our input placeholder  \n",
        "    input_img = Input(shape=(len_feature,))\n",
        "\n",
        "    # coding layers  \n",
        "    encoded = Dense(1280, activation='relu', name='encode_layer1')(input_img)\n",
        "    encoded = Dense(320, activation='relu', name='encode_layer2')(encoded)\n",
        "    encoded = Dense(128, activation='relu', name='encode_layer3')(encoded)\n",
        "    encoded = Dense(64, activation='relu', name='encode_layer4')(encoded)\n",
        "    encoder_output = Dense(encoding_dim)(encoded)\n",
        "\n",
        "    # decoding layers  \n",
        "    decoded = Dense(64, activation='relu', name='decode_layer1')(encoder_output)\n",
        "    decoded = Dense(128, activation='relu', name='decode_layer2')(decoded)\n",
        "    decoded = Dense(320, activation='relu', name='decode_layer3')(decoded)\n",
        "    decoded = Dense(1280, activation='relu', name='decode_layer4')(decoded)\n",
        "    decoded = Dense(len_feature, activation='tanh')(decoded)\n",
        "\n",
        "    # autoencoder model \n",
        "    autoencoder = Model(inputs=input_img, outputs=decoded)\n",
        "  \n",
        "    # encoder model\n",
        "    # encoder = Model(inputs=input_img, outputs=encoder_output)\n",
        "\n",
        "    # compile autoencoder  \n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "    \n",
        "    autoencoder.load_weights(filename_pretrained_weights)\n",
        "    \n",
        "    # obtained the weights\n",
        "    l1 = autoencoder.get_layer('encode_layer1').get_weights()\n",
        "    l2 = autoencoder.get_layer('encode_layer2').get_weights()\n",
        "    l3 = autoencoder.get_layer('encode_layer3').get_weights()\n",
        "    l4 = autoencoder.get_layer('encode_layer4').get_weights()\n",
        "\n",
        "    # Output weights\n",
        "    a1 = l1[0]\n",
        "    a2 = l2[0]\n",
        "    a3 = l3[0]\n",
        "    a4 = l4[0]\n",
        "\n",
        "    final_weights = np.dot(np.dot(np.dot(a1, a2), a3), a4) # Genes x n_pcs\n",
        "    final_weights = final_weights - np.min(final_weights) # Genes x n_pcs\n",
        "    \n",
        "    n_genes = final_weights.shape[0]\n",
        "    n_comps = final_weights.shape[1]\n",
        "\n",
        "    for i in range(n_genes):\n",
        "        max_weight = np.max(final_weights[i,:])\n",
        "    \n",
        "        for j in range(n_comps):\n",
        "            if final_weights[i,j] != max_weight:\n",
        "                final_weights[i,j] = 0\n",
        "        #end_for_j\n",
        "  \n",
        "    #end_for_i\n",
        "    \n",
        "    # return matrix is nComps x Genes\n",
        "    return final_weights.T "
      ],
      "metadata": {
        "id": "ItS2x3hQbfcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the results from Topic model\n",
        "# best_model_float should be float matrix with ntopics*genes\n",
        "def Topicmodel_Results_Compile(best_model_float, nodecut = 500):\n",
        "    \n",
        "    shapeTopics = best_model_float.shape\n",
        "    \n",
        "    indexarray = np.argsort(-best_model_float,axis=1)\n",
        "    \n",
        "    regroup_indexarray = []  # list\n",
        "    first_layer = [nodecut]*shapeTopics[0]     # list\n",
        "    \n",
        "    for i in range(shapeTopics[0]):\n",
        "        tpidx = indexarray[i]\n",
        "        regroup_indexarray.append(tpidx[0:nodecut])\n",
        "    #end_for_i\n",
        "    \n",
        "    #regroup_indexarray(list); first_layer(list) \n",
        "    return regroup_indexarray, first_layer   "
      ],
      "metadata": {
        "id": "ag3QdNWVblRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the results from FastICA and AutoEncoder\n",
        "# best_model_float should be float matrix with ntopics*genes\n",
        "def Results_Compile(best_model_float, th = 1):\n",
        "    \n",
        "    shapeTopics = best_model_float.shape\n",
        "    \n",
        "    indexarray = np.argsort(-best_model_float,axis=1)\n",
        "    regroup_indexarray = []  # list\n",
        "    first_layer = []     # list\n",
        "    \n",
        "    for i in range(shapeTopics[0]):\n",
        "        if np.std(best_model_float[i]) !=0:\n",
        "            tpidx = indexarray[i]\n",
        "            tp = best_model_float[i, tpidx]\n",
        "            t_total = sum(tp)\n",
        "            \n",
        "            for j in range(1, shapeTopics[1]):\n",
        "                tp_acum = sum(tp[0:j])/t_total\n",
        "                if tp_acum >= th:\n",
        "                    regroup_indexarray.append(tpidx[0:j])\n",
        "                    first_layer.append(j)\n",
        "                    break\n",
        "                #end_if\n",
        "            #end_for_j\n",
        "        #end_if\n",
        "    #end_for_i\n",
        "    \n",
        "    #regroup_indexarray(list); first_layer(list) \n",
        "    return regroup_indexarray, first_layer "
      ],
      "metadata": {
        "id": "a7eODY1SbpHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# log2 transformed gene exp array\n",
        "# input array should be genes*samples\n",
        "def Log_transform(exp_matrix):\n",
        "    \n",
        "    n_samples = exp_matrix.shape[1]\n",
        "    n_gene = exp_matrix.shape[0]\n",
        "    \n",
        "    for i in range(n_samples):\n",
        "        if i ==0:\n",
        "            tp = exp_matrix[:,i]\n",
        "            log_tran_matrix = np.log2(tp)\n",
        "            log_tran_matrix = log_tran_matrix.reshape(n_gene, 1)\n",
        "        else:\n",
        "            tp = exp_matrix[:,i]\n",
        "            tp = np.log2(tp)\n",
        "            tp = tp.reshape(n_gene,1)\n",
        "            log_tran_matrix = np.concatenate((log_tran_matrix, tp), axis = 1)\n",
        "    \n",
        "    return log_tran_matrix"
      ],
      "metadata": {
        "id": "_iMb5Mv6bs0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct TreeMap for a sample\n",
        "# exp_vector is a gene expression vector for a sample\n",
        "# top_layer_list, gene_im_list can be obtained from ResultsCompile_* functions\n",
        "def Draw_Treemap(exp_vector, top_layer_list, gene_imp_list, im_name = '_tmp_fig.png', high_inch = 5, width_inch = 5, set_dpi = 200):\n",
        "    \n",
        "    #exp_vector = np.log2(exp_vector)\n",
        "    \n",
        "    exp_gene_list = []\n",
        "    for i in range(len(top_layer_list)):\n",
        "        tp_exp = exp_vector[gene_imp_list[i]]\n",
        "        exp_gene_list.append(tp_exp)\n",
        "    #end_for_i\n",
        "    \n",
        "    x = 0\n",
        "    y = 0\n",
        "    width = 1\n",
        "    height = 1\n",
        "    \n",
        "    mini = np.min(exp_vector)\n",
        "    maxi = np.max(exp_vector)\n",
        "    #mini = 0.001\n",
        "    #maxi = 30000\n",
        "\n",
        "    cmap = matplotlib.cm.jet\n",
        "    norm = matplotlib.colors.Normalize(vmin=mini, vmax=maxi)\n",
        "\n",
        "    tp_array = np.array(top_layer_list)\n",
        "    ori_topic_idx = np.argsort(-tp_array)\n",
        "    \n",
        "    top_layer_list.sort(reverse=True)\n",
        "    first_values = squarify.normalize_sizes(top_layer_list, width, height)\n",
        "    \n",
        "    # Start to prepare figure\n",
        "    fig = plt.figure(frameon = False)\n",
        "    fig.set_size_inches(width_inch, high_inch)\n",
        "\n",
        "    rects = squarify.squarify(first_values, x, y, width, height)\n",
        "    \n",
        "    for i in range(len(top_layer_list)):\n",
        "        tp_ax = plt.Axes(fig, [rects[i]['x'], rects[i]['y'], rects[i]['dx'], rects[i]['dy']], )\n",
        "        tplen = top_layer_list[i]\n",
        "        tplist = [1]*tplen\n",
        "        tp_values = squarify.normalize_sizes(tplist, rects[i]['dx'], rects[i]['dy'])\n",
        "        \n",
        "        mycolor = [cmap(norm(value)) for value in exp_gene_list[ori_topic_idx[i]]]\n",
        "        tp_ax = squarify.plot(tp_values, color=mycolor, ax=tp_ax)\n",
        "        tp_ax.set_axis_off()\n",
        "        fig.add_axes(tp_ax)\n",
        "    \n",
        "    #end_for_i\n",
        "    plt.savefig(im_name, dpi = set_dpi)\n",
        "    plt.cla()\n",
        "    plt.close('all')\n",
        "    \n",
        "    return 0"
      ],
      "metadata": {
        "id": "pXQD7V8Gbxbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the images for training and validation\n",
        "# dataset should be genes x samples\n",
        "def Batch_ImageOrganizor(dataset, datalabel, top_layer_list, gene_imp_list, path = './data/', dirname = 'Dataset'):\n",
        "    \n",
        "    n_class = np.unique(datalabel)\n",
        "    curr_path = os.getcwd()\n",
        "    generate_path = path+dirname\n",
        "    \n",
        "    if os.path.exists(generate_path) == False:\n",
        "        os.makedirs(generate_path)\n",
        "        \n",
        "        for i in range(len(n_class)):\n",
        "            os.chdir(curr_path)\n",
        "            Classname = 'C'+np.str(i+1).zfill(3)\n",
        "            os.makedirs(generate_path+'/'+Classname)\n",
        "            os.chdir(generate_path+'/'+Classname)\n",
        "            \n",
        "            idx_samples = np.argwhere(datalabel == i)\n",
        "            num_samples = idx_samples.shape[0]\n",
        "            idx_samples = idx_samples.reshape(num_samples)\n",
        "\n",
        "            exp_data_inclass = dataset[:,idx_samples]\n",
        "                    \n",
        "            for j in range(num_samples):\n",
        "                tp_exp = exp_data_inclass[:,j]\n",
        "                im_file_name = Classname+'_P'+str(j+1).zfill(5)+'.jpg'\n",
        "                \n",
        "                #ex\n",
        "                if j==0:\n",
        "                    tp=Draw_Treemap(tp_exp, top_layer_list, gene_imp_list, im_name = im_file_name)\n",
        "\n",
        "                tp = Draw_Treemap(tp_exp, top_layer_list, gene_imp_list, im_name = im_file_name)\n",
        "            #end_for_j\n",
        "        #end_for_i\n",
        "    else:\n",
        "        print('The path: <'+generate_path+'> exists!')\n",
        "    \n",
        "    os.chdir(curr_path)\n",
        "    return 0"
      ],
      "metadata": {
        "id": "IrmrymRfb1KW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly Split samples from the whole-training set into sub_train and sub_test\n",
        "# According to the split_rate    \n",
        "def Sample_Split(fromdir = './data/Train', split_rate = 0.3):\n",
        "    \n",
        "    if (os.path.exists('./split_data')):\n",
        "        shutil.rmtree('./split_data')\n",
        "    \n",
        "    todir_train = './split_data/Train'\n",
        "    todir_test = './split_data/Test'\n",
        "    \n",
        "    all_class = os.listdir(fromdir)\n",
        "    \n",
        "    np.random.seed(100)\n",
        "    \n",
        "    for i in range(len(all_class)):\n",
        "        \n",
        "        curr_path = fromdir+'/'+all_class[i]\n",
        "        all_files = os.listdir(curr_path)\n",
        "        \n",
        "        to_path_train = todir_train+'/'+all_class[i]\n",
        "        to_path_test = todir_test+'/'+all_class[i]\n",
        "        os.makedirs(to_path_train)\n",
        "        os.makedirs(to_path_test)\n",
        "        \n",
        "        topn = round(len(all_files)*split_rate)\n",
        "        idx = np.arange(0,len(all_files))\n",
        "        idx = np.random.permutation(idx)\n",
        "        \n",
        "        for j in range(len(idx)):\n",
        "            \n",
        "            if (j<=topn):\n",
        "                from_file = curr_path+'/'+all_files[idx[j]]\n",
        "                to_file = to_path_test+'/'+all_files[idx[j]]\n",
        "                shutil.copyfile(from_file, to_file)\n",
        "            else:\n",
        "                from_file = curr_path+'/'+all_files[idx[j]]\n",
        "                to_file = to_path_train+'/'+all_files[idx[j]]\n",
        "                shutil.copyfile(from_file, to_file)\n",
        "            #end_if\n",
        "        #end_for_j\n",
        "\n",
        "    #end_for_i\n",
        "    \n",
        "    return 0"
      ],
      "metadata": {
        "id": "QiOEyIjrb4jI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a CNN model\n",
        "def MiniModel(input_shape, n_class):\n",
        "    images = Input(input_shape)\n",
        "    net = Unit(images,32)\n",
        "    net = MaxPooling2D(pool_size=(2,2))(net)\n",
        "    net = Unit(net,64)\n",
        "    net = MaxPooling2D(pool_size=(2,2))(net)\n",
        "    net = Unit(net,128)\n",
        "    net = MaxPooling2D(pool_size=(2,2))(net)\n",
        "    #net = Unit(net,256)\n",
        "    #net = MaxPooling2D(pool_size=(2,2))(net)\n",
        "    #net = Unit(net,512)\n",
        "    #net = AveragePooling2D(pool_size=(2,2))(net)\n",
        "    net = Flatten()(net)\n",
        "    #net = BatchNormalization()(net)\n",
        "    net = Dense(units=256,activation=\"selu\")(net)\n",
        "    net = Dropout(0.5)(net)\n",
        "    net = Dense(units=n_class,activation=\"softmax\")(net)\n",
        "    model = Model(inputs=images,outputs=net)\n",
        "    return model\n",
        "\n",
        "    def CNN_Construction(train_dir = './data/Train', val_dir = './data/Validation', target_w = 100, target_h =100, batchsize = 32, ranseed = 100, n_epochs = 100):\n",
        "    \n",
        "    train_datagen = ImageDataGenerator(rescale = 1/255.,\n",
        "                                       rotation_range=0,\n",
        "                                       width_shift_range=0,\n",
        "                                       height_shift_range=0,\n",
        "                                       shear_range=0,\n",
        "                                       zoom_range=0,\n",
        "                                       fill_mode='nearest')\n",
        "    val_datagen = ImageDataGenerator(rescale = 1/255.)\n",
        "    \n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "                                        train_dir,\n",
        "                                        target_size=(target_w, target_h),\n",
        "                                        batch_size=batchsize,\n",
        "                                        seed = ranseed,\n",
        "                                        color_mode='rgb',\n",
        "                                        class_mode='categorical')\n",
        "    val_generator = val_datagen.flow_from_directory(\n",
        "                                        val_dir,\n",
        "                                        target_size=(target_w, target_h),\n",
        "                                        batch_size=batchsize,\n",
        "                                        seed = ranseed,\n",
        "                                        color_mode='rgb',\n",
        "                                        class_mode='categorical')\n",
        "    \n",
        "    n_class = train_generator.num_classes\n",
        "    n_train = train_generator.n\n",
        "    n_val = val_generator.n\n",
        "    \n",
        "    input_shape = (target_h, target_w, 3)\n",
        "    \n",
        "    model = MiniModel(input_shape, n_class)\n",
        "    sgd = SGD(lr=0.0001, momentum=0.9, decay=0.001/n_epochs, nesterov=False)\n",
        "    #sgd = SGD(learning_rate=0.0001)\n",
        "    model.compile(optimizer=sgd,loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
        "    \n",
        "    \n",
        "    best_AUC = 0\n",
        "    best_ACC = 0\n",
        "    \n",
        "    best_model_filepath = './saved_best_model.hdf5'\n",
        "    \n",
        "    #new start\n",
        "    checkpointer = ModelCheckpoint(filepath=best_model_filepath, monitor='val_accuracy', verbose=2, save_best_only=True, mode='max')\n",
        "    \n",
        "    fitted_model = model.fit(\n",
        "                                train_generator,\n",
        "                                steps_per_epoch=n_train//batchsize,\n",
        "                                epochs=n_epochs,\n",
        "                                validation_data=val_generator,\n",
        "                                validation_steps=n_val//batchsize,\n",
        "                                verbose=1, \n",
        "                                callbacks=[checkpointer])\n",
        "       \n",
        "    # Test the model in training sets\n",
        "    model = load_model(best_model_filepath)\n",
        "    \n",
        "    ld_time = int(np.ceil(n_val/batchsize))\n",
        "    \n",
        "    for i in range(ld_time):\n",
        "        if (i == 0):\n",
        "            data, val_true_label = val_generator.next()\n",
        "        else:\n",
        "            data, tplabel = val_generator.next()\n",
        "            val_true_label = np.concatenate((val_true_label, tplabel), axis = 0)\n",
        "    #end_for_i\n",
        "    val_true_label = val_true_label.argmax(1)\n",
        "    \n",
        "    val_predict = model.predict(val_generator)\n",
        "    val_predict = val_predict.argmax(1)\n",
        "    best_AUC = metrics.roc_auc_score(val_true_label, val_predict)\n",
        "    best_ACC = metrics.accuracy_score(val_true_label, val_predict)\n",
        "    \n",
        "    return best_AUC, best_ACC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "auTFhci0b78E",
        "outputId": "2a71fc54-bde7-49af-b912-804306edd79a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-1cecc0ee826e>\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    train_datagen = ImageDataGenerator(rescale = 1/255.,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using trained CNN weights for predicion    \n",
        "def CNN_Prediction(model_file = './saved_best_model.hdf5', val_dir = './data/Validation', target_w = 100, target_h =100, batchsize = 32, ranseed = 100, n_epochs = 100):\n",
        "    \n",
        "    val_datagen = ImageDataGenerator(rescale = 1/255.)\n",
        "    \n",
        "    val_generator = val_datagen.flow_from_directory(\n",
        "                                        val_dir,\n",
        "                                        target_size=(target_w, target_h),\n",
        "                                        batch_size=batchsize,\n",
        "                                        seed = ranseed,\n",
        "                                        color_mode='rgb',\n",
        "                                        class_mode='categorical')\n",
        "    \n",
        "    n_val = val_generator.n\n",
        "    model = load_model(model_file)\n",
        "    \n",
        "    ld_time = int(np.ceil(n_val/batchsize))\n",
        "    \n",
        "    for i in range(ld_time):\n",
        "        if (i == 0):\n",
        "            data, val_true_label = val_generator.next()\n",
        "        else:\n",
        "            data, tplabel = val_generator.next()\n",
        "            val_true_label = np.concatenate((val_true_label, tplabel), axis = 0)\n",
        "    #end_for_i\n",
        "    val_true_label = val_true_label.argmax(1)\n",
        "    \n",
        "    val_predict = model.predict(val_generator)\n",
        "    val_predict = val_predict.argmax(1)\n",
        "    \n",
        "    # output\n",
        "    print(\"ACC Score (Validation):  %5.4f\"%metrics.accuracy_score(val_true_label, val_predict))\n",
        "    print (\"AUC Score (Validation):  %5.4f\"%metrics.roc_auc_score(val_true_label,val_predict))\n",
        "    pred_mcc = metrics.matthews_corrcoef(val_true_label, val_predict)\n",
        "    print (\"MCC Score (Validation):  %5.4f\"%pred_mcc)\n",
        "\n",
        "    tn, fp, fn, tp = metrics.confusion_matrix(val_true_label, val_predict, labels=[0, 1]).ravel()\n",
        "    print(\"Validation: (TP, FP, TN, FN):  %d, %d, %d, %d\"%(tp, fp, tn, fn))\n",
        "    \n",
        "    pred_acc = metrics.accuracy_score(val_true_label, val_predict)\n",
        "    pred_AUC = metrics.roc_auc_score(val_true_label, val_predict)\n",
        "    return pred_acc, pred_AUC, pred_mcc\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  \n",
        "    # constants\n",
        "    looptime = 1\n",
        "    methods = 2         # 0 - Autoencoder; 1 - FastICA; 2 - Topic Model\n",
        "    datafile = './Dataset_SEQC_NB_RPM_GSE62564_EFS.xlsx'\n",
        "    \n",
        "    if (methods == 0):\n",
        "        #best_model_float = Auto_Encoder(all_data)\n",
        "        #regroup_indexarray, first_layer = Results_Compile(best_model_float)\n",
        "        custom_path = './imgdata_Autoencoder/'\n",
        "    elif (methods == 1):\n",
        "        #best_model_float = FastICA_Model(all_data)\n",
        "        #regroup_indexarray, first_layer = Results_Compile(best_model_float)\n",
        "        custom_path = './imgdata_FastICA/'\n",
        "    else:\n",
        "        #all_data = all_data*100\n",
        "        #all_data = all_data.astype(np.int)\n",
        "        #best_model_float, perlist = Topic_Model(all_data)\n",
        "        #savedata = best_model_float.T\n",
        "        #tp = np.savetxt('_temp_best_model_matrix.csv', savedata, delimiter=',')\n",
        "        #ps = np.array(perlist)\n",
        "        #ps = ps.T\n",
        "        #tp = np.savetxt('_temp_all_perplexity.csv', ps, delimiter=',')\n",
        "        #regroup_indexarray, first_layer = Topicmodel_Results_Compile(best_model_float, nodecut = 600)\n",
        "        custom_path = './imgdata_TopicModel/'\n",
        "    \n",
        "    res = [['train_acc', 'train_auc', 'val_acc', 'val_auc', 'val_mcc']]\n",
        "    for i in range(looptime):\n",
        "        frompath = custom_path+'Train'    \n",
        "        tp = Sample_Split(fromdir = frompath, split_rate = 0.1)\n",
        "    \n",
        "        validationdir = custom_path+'Validation'\n",
        "        \n",
        "        best_AUC, best_ACC = CNN_Construction(train_dir = './split_data/Train', val_dir = './split_data/Test',target_w = 256, target_h =256, batchsize = 20, n_epochs = 100)\n",
        "        print(\"AUC Score (Training):  %5.4f\"%best_AUC)\n",
        "        print(\"ACC Score (Training):  %5.4f\"%best_ACC)\n",
        "        pred_acc, pred_AUC, pred_mcc = CNN_Prediction(model_file = './saved_best_model.hdf5', val_dir = validationdir, target_w = 256, target_h =256, batchsize = 20, n_epochs = 100)\n",
        "    \n",
        "        tpres = [best_ACC, best_AUC, pred_acc, pred_AUC, pred_mcc]\n",
        "        res.append(tpres)\n",
        "        \n",
        "    with open(\"Loop_results.txt\",\"w\") as f:\n",
        "        for i in res:\n",
        "            i = str(i).strip('[').strip(']').replace('\\'','')+'\\n'\n",
        "            f.writelines(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "TZNlYHj9cQFT",
        "outputId": "0de6d892-9c95-4227-9b1d-7f1f0b28bd3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-06504be951bf>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlooptime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mfrompath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSample_Split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfromdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrompath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mvalidationdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Validation'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-33e45ad7f11d>\u001b[0m in \u001b[0;36mSample_Split\u001b[0;34m(fromdir, split_rate)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtodir_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./split_data/Test'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mall_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfromdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './imgdata_TopicModel/Train'"
          ]
        }
      ]
    }
  ]
}